\pdfbookmark[0]{Abstract}{abstract}
\begin{center} 
\huge Abstract
\end{center}


This thesis concentrates on implementing and evaluating different distribution algorithms for large data transfers in heterogenous peer-to-peer networks, especially for incremental transfers, which yields the possibillity for peer-to-peer video streaming. The main focus is on 1:N scenarios where only one peer has a given data set completely and N peers try to distribute this data set as fast as possible, though other scenarios are implemented and evaluated as well. From now on I call the peer which has the data set completely peerZero.

The main problem is the choice of the best distribution algorithm, which should be able to use most of the available network bandwidth of all participating peers. In a traditional client/server system the server uploads the data to all clients sequentially, which means that only the server upload bandwidth is used but none of the client upload bandwidths.

Peer-to-peer networks in combination with efficient distribution algorithms can help to solve this problem. Those algorithms are always based to the same technique where all participating peers load specific chunks from peerZero and distribute those chunks among themselves. This way the upload bandwidths of the peers are not idle. The difficulty is the choice of the specific chunks the peers download and the tuning of parameters like chunk count and chunk size. Good algorithms are also flexible enough to adjust themselves to changing network conditions.

The application is implemented in Java using the Netty framework, a high performance network library. This framework offers the possibillity to run simulated network connections which is great for monitoring and evaluating different distribution algorithms since real networks add too much overhead for testing. The application implements a variation of the BitTorrent protocol which I call "Intelligent Brain", a logarithmic distribution algorithm and a client/server algorithm for comparison. The test results show that the "Intelligent Brain" is able to almost fully load all participating peers which means that if a transfer from A->B takes T seconds, the time taken for the complete distribution for N peers also is about T seconds, in practice it is often 1.5 * T seconds. The delay is caused by meta data overhead.
