\pdfbookmark[0]{Abstract}{abstract}
\begin{center} 
\huge Abstract
\end{center}


This thesis concentrates on implementing and evaluating different distribution algorithms for large data transfers in heterogenous peer-to-peer networks, especially for incremental transfers, which yields the possibillity for peer-to-peer video streaming. The main focus is on 1:N scenarios where only one peer has a given data set completely and N peers try to distribute this data set as fast as possible, though other scenarios are implemented and evaluated as well.

The main problem is the choice of the best distribution algorithm, which should be able to use most of the available network bandwidth of all participating peers. In a traditional client/server system the server uploads the data to all clients sequentially, which means that only the server upload bandwidth is used but none of the client upload bandwidths.

Peer-to-peer networks in combination with efficient distribution algorithms can help to solve this problem. Those algorithms are always based to the same technique where all participating peers load specific chunks from the peer with the whole data set and distribute those chunks among themselves. This way the upload bandwidths of the peers are not idle. The difficulty is the choice of the specific chunks the peers download and the tuning of parameters like chunk count and chunk size. Good algorithms are also flexible enough to adjust themselves to changing network conditions.

In the course of this thesis an application was developed which implements a variation of the BitTorrent protocol, I call this variation Chunked-Swarm-Distribution (CSD), which is better suited for the needs of this thesis in terms of performance and time, a logarithmic distribution algorithm and a sequential client/server disitribution algorithm for comparison. The application is implemented in Java using the Netty framework, a high performance network library. This framework offers the possibillity to run simulated network connections which is great for monitoring and evaluating different distribution algorithms since real networks add too much overhead for testing. The application implements a couple of comparing benchmarks to evaluate the quality of the algorithms used.

The benchmark results show that the CSD algorithm is able to almost fully load all participating peers which means that if a transfer from peer A to peer B takes T seconds, the time taken for the complete distribution for N peers also is about T seconds, in practice it is often a bit over T seconds, which is caused by meta data overhead. This means that the number of participating peers is, theoretically, of no importance. In practice more peers means more meta data, but with compression and efficient data formats those influences can be minimised.
